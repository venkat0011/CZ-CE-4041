{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# using the saturation values and hue the values we are able to segment the leafs, create a fucntion to make to do the segmentation\ndef segment_image(image):\n    # convert the image to hue \n    img_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    # extract the green colour from the hue\n    lower_mask_hue = img_hsv [:,:,0] >= 25\n    upper_mask_hue = img_hsv [:,:,0] <= 75\n    saturation_lower = img_hsv [:,:,1] >= 40\n    saturation_upper = img_hsv[:,:,1] <= 255\n    value_lower = img_hsv[:,:,2]>= 50\n    value_upper  = img_hsv[:,:,2]<=255\n    mask = lower_mask_hue*upper_mask_hue*saturation_lower*saturation_upper*value_lower*value_upper\n    red = img_hsv[:,:,0]*mask\n    green = img_hsv[:,:,1]*mask\n    blue = img_hsv[:,:,2]*mask\n    mask = np.dstack((red,green,blue))\n    mask = cv2.normalize(mask, None, alpha=0,beta =200, norm_type=cv2.NORM_MINMAX)\n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:01:04.153144Z","iopub.execute_input":"2022-04-10T02:01:04.153769Z","iopub.status.idle":"2022-04-10T02:01:04.161938Z","shell.execute_reply.started":"2022-04-10T02:01:04.153728Z","shell.execute_reply":"2022-04-10T02:01:04.160539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport cv2\nfrom glob import glob\n# Input data files are available in the read-only \"../input/\" directory\ndef load_images(path):\n    images = []\n    labels = []\n    for dirname, folders,_ in os.walk(path):\n        for folder in folders:\n            path = os.path.join(dirname,folder)\n            for image_path in glob(os.path.join(path, \"*.png\")):\n                image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n                image = segment_image(image)\n                image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n                images.append(image)\n                labels.append(folder)\n    return images,labels\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:01:04.164309Z","iopub.execute_input":"2022-04-10T02:01:04.165102Z","iopub.status.idle":"2022-04-10T02:01:04.539528Z","shell.execute_reply.started":"2022-04-10T02:01:04.165048Z","shell.execute_reply":"2022-04-10T02:01:04.53839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import np_utils\nfrom sklearn import preprocessing\nimages,labels = load_images('/kaggle/input/plant-seedlings-classification/train')\nlabels = pd.DataFrame(labels)\nle = preprocessing.LabelEncoder()\nle.fit(labels[0])\nencodeTrainLabels = le.transform(labels[0])\nclasses = np_utils.to_categorical(encodeTrainLabels)\nimages = np.asarray(images)\nclasses = np.asarray(classes).astype('int').reshape((4750,12))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:01:04.540862Z","iopub.execute_input":"2022-04-10T02:01:04.541118Z","iopub.status.idle":"2022-04-10T02:03:24.464969Z","shell.execute_reply.started":"2022-04-10T02:01:04.54109Z","shell.execute_reply":"2022-04-10T02:03:24.464011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(images, classes, \n                                                test_size=0.2, random_state=2022, \n                                                stratify = classes)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:03:24.466909Z","iopub.execute_input":"2022-04-10T02:03:24.467495Z","iopub.status.idle":"2022-04-10T02:03:25.105522Z","shell.execute_reply.started":"2022-04-10T02:03:24.467457Z","shell.execute_reply":"2022-04-10T02:03:25.104668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ninput_model = tf.keras.layers.Input(shape = (224, 224, 3), name = 'image_input')\n\n# main model with Incpetion - ResNet - v2 layers\n# omit the top layers as we are adding custom layers\n# use transfer learning, with weights from Imagenet trained model\nmain_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top = False, weights = 'imagenet')(input_model)\n\n# flatten model to get appropriate dimensions\nflattened_model = tf.keras.layers.Flatten()(main_model)\n\n# add custom dropout and dense layers\ndropout_1 = tf.keras.layers.Dropout(0.5)(flattened_model)\ndense_1 = tf.keras.layers.Dense(128, activation = 'relu', activity_regularizer=tf.keras.regularizers.l2(1e-5))(dropout_1)\ndropout_2 = tf.keras.layers.Dropout(0.5)(dense_1)\n\n# output of model\noutput_model = tf.keras.layers.Dense(12, activation = \"softmax\", activity_regularizer=tf.keras.regularizers.l2(1e-5))(dropout_2)\n\nmodel = tf.keras.models.Model(input_model,  output_model)\n\n# use Adam optimizer with model\noptimizer = tf.keras.optimizers.Adam(lr = 5e-4, beta_1 = 0.9, beta_2 = 0.999)\n\n# use categorical crossentropy loss since classification task\nmodel.compile(loss=\"categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:03:34.862891Z","iopub.execute_input":"2022-04-10T02:03:34.863379Z","iopub.status.idle":"2022-04-10T02:03:44.164478Z","shell.execute_reply.started":"2022-04-10T02:03:34.863344Z","shell.execute_reply":"2022-04-10T02:03:44.163404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        rotation_range=180,  # randomly rotate images in the range\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally\n        height_shift_range=0.1,  # randomly shift images vertically \n        horizontal_flip=True,  # randomly flip images horizontally\n        vertical_flip=True  # randomly flip images vertically\n    )  \ndatagen.fit(trainX)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:03:57.936682Z","iopub.execute_input":"2022-04-10T02:03:57.93737Z","iopub.status.idle":"2022-04-10T02:04:01.529892Z","shell.execute_reply.started":"2022-04-10T02:03:57.937331Z","shell.execute_reply":"2022-04-10T02:04:01.529021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def callbacks():\n    \n    # save best model regularly\n    save_best_model = tf.keras.callbacks.ModelCheckpoint(filepath = 'incepresnet',\n        monitor = 'val_acc', save_best_only = True, verbose = 1)\n\n    # reduce learning rate when it stops decreasing\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', factor = 0.4,\n                              patience = 3, min_lr = 1e-10, verbose = 1, cooldown = 1)\n    \n    # stop training early if no further improvement\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor = 'loss', min_delta = 1e-2, patience = 8, verbose = 1,\n        mode = 'min', baseline = None, restore_best_weights = True\n    )\n\n    return [save_best_model, reduce_lr, early_stopping]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:04:18.134598Z","iopub.execute_input":"2022-04-10T02:04:18.134924Z","iopub.status.idle":"2022-04-10T02:04:18.141429Z","shell.execute_reply.started":"2022-04-10T02:04:18.134884Z","shell.execute_reply":"2022-04-10T02:04:18.140696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incep_resnet = model.fit(datagen.flow(trainX,trainY),validation_data=(testX,testY), steps_per_epoch = 110, epochs =15,callbacks = callbacks())","metadata":{"execution":{"iopub.status.busy":"2022-04-10T02:04:20.643706Z","iopub.execute_input":"2022-04-10T02:04:20.644529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing\n\nimages_test = []\ntestId = []\nfor image_path in glob(os.path.join(\"../input/plant-seedlings-classification/test\", \"*.png\")):\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    image = segment_image(image)\n    image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n    images_test.append(image)\n    testId.append(image_path.split('/')[-1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_test = np.asarray(images_test)\nprediction = model.predict(images_test)\npred_number= np.argmax(prediction, axis=1)\npredStr = le.classes_[pred_number]\nres = {'file': testId, 'species': predStr}\nres = pd.DataFrame(res)\nres.to_csv(\"res.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}